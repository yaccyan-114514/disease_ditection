"""
æ¨¡å‹è°ƒè¯•è„šæœ¬ - ç”¨äºè¯Šæ–­æ¨¡å‹é¢„æµ‹é—®é¢˜
"""
import os
import json
import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
from tensorflow.keras import layers as L
from tensorflow.keras.applications import MobileNetV3Small

# é…ç½®
IMG_SIZE = (160, 160)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
WEIGHTS_PATH = os.path.join(SCRIPT_DIR, "streamlit_app", "model", "best_model_tuned.weights.h5")
CLASS_NAMES_PATH = os.path.join(SCRIPT_DIR, "streamlit_app", "model", "class_names.json")

BEST = {
    "use_data_augmentation": False,
    "dense_units": 256,
    "dropout": 0.1,
    "activation": "swish"
}

def load_class_names(path: str):
    """åŠ è½½ç±»åˆ«åç§°"""
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            names = json.load(f)
        return names
    return None

def build_inference_model(num_classes: int, img_size=IMG_SIZE) -> tf.keras.Model:
    """æ„å»ºæ¨ç†æ¨¡å‹"""
    base = MobileNetV3Small(
        input_shape=(img_size[0], img_size[1], 3),
        include_top=False,
        weights=None
    )
    base.trainable = False

    inp = L.Input(shape=(img_size[0], img_size[1], 3), name="input")
    x = base(inp)
    x = L.GlobalAveragePooling2D(name="gap")(x)
    x = L.Dropout(BEST["dropout"], name="dropout")(x)

    kernel_init = "glorot_uniform" if BEST["activation"] == "swish" else "he_normal"

    x = L.Dense(
        BEST["dense_units"],
        use_bias=False,
        kernel_initializer=kernel_init,
        name=f"dense{BEST['dense_units']}"
    )(x)
    x = L.BatchNormalization(name="bn")(x)
    act = tf.keras.activations.swish if BEST["activation"] == "swish" else tf.keras.activations.relu
    x = L.Activation(act, name=BEST["activation"])(x)

    out = L.Dense(num_classes, activation="softmax", dtype="float32", name="pred")(x)
    return tf.keras.Model(inp, out, name="leaf_inference")

def preprocess_image_pil(pil_img: Image.Image, img_size=IMG_SIZE) -> np.ndarray:
    """é¢„å¤„ç†å›¾ç‰‡"""
    img = pil_img.convert("RGB").resize(img_size)
    arr = np.asarray(img, dtype=np.float32)
    arr = preprocess_input(arr)
    return np.expand_dims(arr, axis=0)

def analyze_model_predictions(image_path: str):
    """åˆ†ææ¨¡å‹å¯¹å›¾ç‰‡çš„é¢„æµ‹ç»“æœ"""
    print("=" * 60)
    print("æ¨¡å‹è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # åŠ è½½ç±»åˆ«åç§°
    class_names = load_class_names(CLASS_NAMES_PATH)
    if class_names is None:
        print("âŒ æ— æ³•åŠ è½½ç±»åˆ«åç§°æ–‡ä»¶")
        return
    
    num_classes = len(class_names)
    print(f"âœ“ ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"âœ“ ç±»åˆ«åˆ—è¡¨: {class_names[:5]}... (æ˜¾ç¤ºå‰5ä¸ª)")
    
    # ç»Ÿè®¡Tomatoç±»åˆ«
    tomato_classes = [i for i, name in enumerate(class_names) if name.startswith("Tomato")]
    print(f"\nğŸ“Š ç±»åˆ«åˆ†æ:")
    print(f"   - Tomatoç›¸å…³ç±»åˆ«æ•°é‡: {len(tomato_classes)}")
    print(f"   - Tomatoç±»åˆ«å æ¯”: {len(tomato_classes)/num_classes*100:.1f}%")
    print(f"   - Tomatoç±»åˆ«ç´¢å¼•: {tomato_classes}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ”„ åŠ è½½æ¨¡å‹...")
    model = build_inference_model(num_classes=num_classes, img_size=IMG_SIZE)
    
    if not os.path.exists(WEIGHTS_PATH):
        print(f"âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {WEIGHTS_PATH}")
        return
    
    try:
        model.load_weights(WEIGHTS_PATH)
        print(f"âœ“ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
    if not os.path.exists(image_path):
        print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        return
    
    print(f"\nğŸ–¼ï¸  åŠ è½½å›¾ç‰‡: {image_path}")
    image = Image.open(image_path)
    x = preprocess_image_pil(image, IMG_SIZE)
    print(f"âœ“ å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼Œå½¢çŠ¶: {x.shape}")
    
    # è¿›è¡Œé¢„æµ‹
    print(f"\nğŸ”® è¿›è¡Œé¢„æµ‹...")
    pred = model.predict(x, verbose=0)[0]
    
    # åˆ†æé¢„æµ‹ç»“æœ
    print(f"\nğŸ“ˆ é¢„æµ‹ç»“æœåˆ†æ:")
    print(f"   - é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶: {pred.shape}")
    print(f"   - é¢„æµ‹æ¦‚ç‡æ€»å’Œ: {np.sum(pred):.6f} (åº”è¯¥æ¥è¿‘1.0)")
    print(f"   - æœ€å¤§æ¦‚ç‡å€¼: {np.max(pred):.6f}")
    print(f"   - æœ€å°æ¦‚ç‡å€¼: {np.min(pred):.6f}")
    print(f"   - æ¦‚ç‡æ ‡å‡†å·®: {np.std(pred):.6f}")
    
    # Top 5 é¢„æµ‹
    top_indices = np.argsort(pred)[::-1][:5]
    print(f"\nğŸ† Top 5 é¢„æµ‹ç»“æœ:")
    for i, idx in enumerate(top_indices):
        class_name = class_names[idx] if idx < len(class_names) else f"Class {idx}"
        confidence = pred[idx]
        is_tomato = class_name.startswith("Tomato")
        marker = "ğŸ…" if is_tomato else "  "
        print(f"   {i+1}. {marker} {class_name}: {confidence*100:.2f}%")
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢„æµ‹éƒ½åå‘Tomato
    tomato_probs = [pred[i] for i in tomato_classes]
    total_tomato_prob = sum(tomato_probs)
    print(f"\nâš ï¸  è¯Šæ–­ç»“æœ:")
    print(f"   - æ‰€æœ‰Tomatoç±»åˆ«çš„æ€»æ¦‚ç‡: {total_tomato_prob*100:.2f}%")
    
    if total_tomato_prob > 0.5:
        print(f"   âš ï¸  è­¦å‘Š: æ¨¡å‹è¿‡åº¦åå‘Tomatoç±»åˆ«ï¼")
        print(f"   ğŸ’¡ å¯èƒ½åŸå› :")
        print(f"      1. è®­ç»ƒæ•°æ®ä¸­Tomatoæ ·æœ¬è¿‡å¤šï¼ˆç±»åˆ«ä¸å¹³è¡¡ï¼‰")
        print(f"      2. æ¨¡å‹è®­ç»ƒä¸å……åˆ†ï¼Œåªå­¦ä¼šäº†è¯†åˆ«Tomato")
        print(f"      3. æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°Tomatoç±»åˆ«")
    else:
        print(f"   âœ“ Tomatoæ¦‚ç‡åœ¨æ­£å¸¸èŒƒå›´å†…")
    
    # æ£€æŸ¥é¢„æµ‹åˆ†å¸ƒæ˜¯å¦è¿‡äºé›†ä¸­
    entropy = -np.sum(pred * np.log(pred + 1e-10))
    max_entropy = np.log(num_classes)
    normalized_entropy = entropy / max_entropy
    
    print(f"\nğŸ“Š é¢„æµ‹åˆ†å¸ƒåˆ†æ:")
    print(f"   - ä¿¡æ¯ç†µ: {entropy:.4f}")
    print(f"   - æœ€å¤§ç†µ: {max_entropy:.4f}")
    print(f"   - å½’ä¸€åŒ–ç†µ: {normalized_entropy:.4f}")
    
    if normalized_entropy < 0.1:
        print(f"   âš ï¸  è­¦å‘Š: é¢„æµ‹åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼ˆç†µå€¼è¿‡ä½ï¼‰ï¼")
        print(f"   ğŸ’¡ æ¨¡å‹å¯èƒ½å¯¹æ‰€æœ‰è¾“å…¥éƒ½é¢„æµ‹åŒä¸€ä¸ªç±»åˆ«")
    elif normalized_entropy > 0.9:
        print(f"   âš ï¸  è­¦å‘Š: é¢„æµ‹åˆ†å¸ƒè¿‡äºå‡åŒ€ï¼ˆç†µå€¼è¿‡é«˜ï¼‰ï¼")
        print(f"   ğŸ’¡ æ¨¡å‹å¯èƒ½æ²¡æœ‰å­¦åˆ°æœ‰æ•ˆçš„ç‰¹å¾")
    else:
        print(f"   âœ“ é¢„æµ‹åˆ†å¸ƒæ­£å¸¸")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python debug_model.py <å›¾ç‰‡è·¯å¾„>")
        print("ç¤ºä¾‹: python debug_model.py test_image.jpg")
        sys.exit(1)
    
    image_path = sys.argv[1]
    analyze_model_predictions(image_path)

